{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a1fc2e",
   "metadata": {},
   "source": [
    "# Stage 1 Ferning Classification – Full Pipeline Notebook\n",
    "\n",
    "**Run cells top to bottom.** This notebook is fully self-contained:\n",
    "\n",
    "| Cell | What it does |\n",
    "|------|--------------|\n",
    "| 1 | Imports & GPU/CPU setup |\n",
    "| 2 | Path resolution (auto-detects your dataset folder) |\n",
    "| 3 | Package check (same as old `setup_verify.py`) |\n",
    "| 4 | **Generate** `master_patch_index.csv` from your `.npy` files |\n",
    "| 5 | **Generate** `fold_splits.csv` (stratified, leakage-free) |\n",
    "| 6 | Data verification (same as old `verify_data.py`) |\n",
    "| 7 | Data loading & preprocessing utilities |\n",
    "| 8 | Generator + model definition |\n",
    "| 9 | Evaluation helper |\n",
    "| 10 | Training loop (cross-validation) |\n",
    "| 11 | Results summary |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea1d6f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Imports & GPU/CPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Fix Windows console encoding\n",
    "if sys.platform == 'win32':\n",
    "    try:\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "print(f\"Python version     : {sys.version.split()[0]}\")\n",
    "print(f\"TensorFlow version : {tf.__version__}\")\n",
    "\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    for gpu in physical_gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not set memory growth: {e}\")\n",
    "    print(f\"Device             : GPU {[g.name for g in physical_gpus]}\")\n",
    "else:\n",
    "    print(\"Device             : CPU (no GPU detected - training will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dc7d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 - Configuration & Path Resolution\n",
    "\n",
    "The notebook searches for your `dataset/` folder automatically in this order:\n",
    "```\n",
    "1. <notebook_dir>/../local/data/     <- README default\n",
    "2. <notebook_dir>/local/data/\n",
    "3. <notebook_dir>/data/\n",
    "4. DATA_ROOT_OVERRIDE                <- set this manually if needed\n",
    "```\n",
    "Your dataset folder should look like:\n",
    "```\n",
    "data/\n",
    "  dataset/\n",
    "    PF/   <- Partial Ferning .npy files\n",
    "    CF/   <- Complete Ferning .npy files\n",
    "    NF/   <- No Ferning .npy files\n",
    "  master_patch_index.csv   <- auto-generated by Cell 4\n",
    "  fold_splits.csv          <- auto-generated by Cell 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0776433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optional manual override ------------------------------------------------\n",
    "# Set this to a string if auto-detection fails, e.g.:\n",
    "#   DATA_ROOT_OVERRIDE = r\"C:/Users/YourName/my_project/local/data\"\n",
    "DATA_ROOT_OVERRIDE = None\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS               = 10\n",
    "BATCH_SIZE           = 32\n",
    "LEARNING_RATE        = 1e-4\n",
    "INPUT_SHAPE          = (64, 64, 3)\n",
    "NUM_CLASSES          = 2\n",
    "N_FOLDS              = 5\n",
    "CLASS_NAMES          = [\"No Ferning\", \"Ferning\"]\n",
    "FERNING_CLASSES      = [\"PF\", \"CF\"]   # classes that map to label=1\n",
    "KNOWN_CLASSES        = [\"PF\", \"CF\", \"NF\"]\n",
    "POSITIVE_CLASS_INDEX = 1\n",
    "THRESHOLD            = 0.5\n",
    "RANDOM_SEED          = 42\n",
    "\n",
    "# ---- Locate notebook directory -----------------------------------------------\n",
    "try:\n",
    "    NOTEBOOK_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# ---- Search for data root ----------------------------------------------------\n",
    "if DATA_ROOT_OVERRIDE:\n",
    "    _candidates = [Path(DATA_ROOT_OVERRIDE)]\n",
    "else:\n",
    "    _candidates = [\n",
    "        NOTEBOOK_DIR.parent / \"local\" / \"data\",\n",
    "        NOTEBOOK_DIR / \"local\" / \"data\",\n",
    "        NOTEBOOK_DIR / \"data\",\n",
    "    ]\n",
    "\n",
    "def _find_data_root(candidates):\n",
    "    \"\"\"Return the first candidate that contains a dataset/ subfolder.\"\"\"\n",
    "    for root in candidates:\n",
    "        if (root / \"dataset\").exists():\n",
    "            return root\n",
    "    return candidates[0]  # fallback: use first candidate, cells will create folders\n",
    "\n",
    "DATA_ROOT         = _find_data_root(_candidates)\n",
    "DATASET_DIR       = DATA_ROOT / \"dataset\"\n",
    "MASTER_INDEX_PATH = DATA_ROOT / \"master_patch_index.csv\"\n",
    "FOLD_SPLITS_PATH  = DATA_ROOT / \"fold_splits.csv\"\n",
    "OUTPUT_DIR        = NOTEBOOK_DIR / \"outputs\"\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Notebook dir      : {NOTEBOOK_DIR}\")\n",
    "print(f\"Data root         : {DATA_ROOT}\")\n",
    "print(f\"Dataset folder    : {DATASET_DIR}  {'[EXISTS]' if DATASET_DIR.exists() else '[NOT FOUND - place .npy files here]'}\")\n",
    "print(f\"Master index      : {'[EXISTS]' if MASTER_INDEX_PATH.exists() else '[will be generated]'}\")\n",
    "print(f\"Fold splits       : {'[EXISTS]' if FOLD_SPLITS_PATH.exists() else '[will be generated]'}\")\n",
    "print(f\"Output dir        : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b6802",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 - Package Check\n",
    "Same dependency check as the old `setup_verify.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4aaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PACKAGE CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "required_packages = {\n",
    "    'numpy':      'numpy',\n",
    "    'pandas':     'pandas',\n",
    "    'tensorflow': 'tensorflow',\n",
    "    'sklearn':    'scikit-learn',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'cv2':        'opencv-python',\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "for module_name, package_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"  [OK] {package_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"  [X]  {package_name}  <- MISSING\")\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n  Install missing packages with:\")\n",
    "    print(f\"    pip install {' '.join(missing_packages)}\")\n",
    "    raise ImportError(f\"Missing packages: {missing_packages}\")\n",
    "else:\n",
    "    print(\"\\n  [OK] All packages installed - ready to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90975f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 - Generate `master_patch_index.csv`\n",
    "\n",
    "Scans `dataset/PF/`, `dataset/CF/`, `dataset/NF/` and builds the index CSV.\n",
    "\n",
    "**Skips generation if the file already exists.** Set `FORCE_REGENERATE = True` to rebuild it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_REGENERATE = False   # set True to rebuild even if CSV already exists\n",
    "\n",
    "def generate_master_index(dataset_dir: Path, output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Walk dataset_dir/PF, dataset_dir/CF, dataset_dir/NF and build\n",
    "    master_patch_index.csv with columns:\n",
    "\n",
    "        sample_id    - derived from filename (everything before the last '_')\n",
    "                       e.g. 'sample001_patch03.npy' -> sample_id = 'sample001'\n",
    "                       If there is no '_', the full stem is the sample_id.\n",
    "        patch_id     - full filename stem\n",
    "        class        - PF / CF / NF\n",
    "        label_stage1 - 1 for PF/CF (Ferning), 0 for NF (No Ferning)\n",
    "        path         - absolute path to the .npy file\n",
    "    \"\"\"\n",
    "    if not dataset_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset folder not found: {dataset_dir}\\n\"\n",
    "            f\"Please create it and place your .npy files in subfolders PF/, CF/, NF/.\"\n",
    "        )\n",
    "\n",
    "    records = []\n",
    "    for class_name in KNOWN_CLASSES:\n",
    "        class_dir  = dataset_dir / class_name\n",
    "        if not class_dir.exists():\n",
    "            print(f\"  [!] Class folder not found, skipping: {class_dir}\")\n",
    "            continue\n",
    "\n",
    "        npy_files = sorted(class_dir.glob(\"*.npy\"))\n",
    "        if not npy_files:\n",
    "            print(f\"  [!] No .npy files found in {class_dir}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  {class_name}: {len(npy_files):,} patches found\")\n",
    "\n",
    "        for fp in npy_files:\n",
    "            stem      = fp.stem\n",
    "            parts     = stem.rsplit(\"_\", 1)\n",
    "            sample_id = parts[0] if len(parts) > 1 else stem\n",
    "\n",
    "            records.append({\n",
    "                \"sample_id\"   : sample_id,\n",
    "                \"patch_id\"    : stem,\n",
    "                \"class\"       : class_name,\n",
    "                \"label_stage1\": 1 if class_name in FERNING_CLASSES else 0,\n",
    "                \"path\"        : str(fp.resolve()),\n",
    "            })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\n",
    "            \"No .npy files found in any class folder. \"\n",
    "            \"Check that your dataset is placed correctly.\"\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n  Saved -> {output_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING master_patch_index.csv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if MASTER_INDEX_PATH.exists() and not FORCE_REGENERATE:\n",
    "    print(f\"  [SKIP] Already exists - loading from disk.\")\n",
    "    print(f\"         Set FORCE_REGENERATE = True to rebuild.\")\n",
    "    master_index = pd.read_csv(MASTER_INDEX_PATH)\n",
    "else:\n",
    "    master_index = generate_master_index(DATASET_DIR, MASTER_INDEX_PATH)\n",
    "\n",
    "print(f\"\\n  Total patches  : {len(master_index):,}\")\n",
    "print(f\"  Unique samples : {master_index['sample_id'].nunique():,}\")\n",
    "print(\"\\n  Class breakdown:\")\n",
    "for cls, count in master_index['class'].value_counts().items():\n",
    "    label = 1 if cls in FERNING_CLASSES else 0\n",
    "    print(f\"    {cls} (label={label}): {count:,} patches\")\n",
    "print(f\"\\n  Stage-1 binary:\")\n",
    "print(f\"    Ferning  (1): {(master_index['label_stage1']==1).sum():,}\")\n",
    "print(f\"    No Fern  (0): {(master_index['label_stage1']==0).sum():,}\")\n",
    "print(\"\\n  Preview:\")\n",
    "display(master_index.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976087ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 - Generate `fold_splits.csv`\n",
    "\n",
    "Creates a **stratified** 5-fold split at the **sample level** — all patches from one\n",
    "sample always land in the same split, which prevents data leakage between folds.\n",
    "\n",
    "**Skips generation if the file already exists.** Set `FORCE_REGENERATE_FOLDS = True` to rebuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_REGENERATE_FOLDS = False   # set True to rebuild even if CSV already exists\n",
    "\n",
    "def generate_fold_splits(\n",
    "    master_index: pd.DataFrame,\n",
    "    output_path: Path,\n",
    "    n_folds: int = 5,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build fold_splits.csv with columns: sample_id, fold, split\n",
    "\n",
    "    Strategy:\n",
    "      - Collapse to one row per sample_id\n",
    "      - Each sample gets its majority class label (for stratification)\n",
    "      - StratifiedKFold ensures each fold has a balanced class ratio\n",
    "      - All patches of a sample inherit that sample's fold assignment\n",
    "    \"\"\"\n",
    "    sample_df = (\n",
    "        master_index\n",
    "        .groupby(\"sample_id\")[\"label_stage1\"]\n",
    "        .agg(lambda x: int(x.mode()[0]))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"label_stage1\": \"majority_label\"})\n",
    "    )\n",
    "\n",
    "    sample_ids = sample_df[\"sample_id\"].values\n",
    "    labels     = sample_df[\"majority_label\"].values\n",
    "\n",
    "    skf     = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    records = []\n",
    "\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(sample_ids, labels), start=1):\n",
    "        for idx in train_idx:\n",
    "            records.append({\"sample_id\": sample_ids[idx], \"fold\": fold_num, \"split\": \"train\"})\n",
    "        for idx in val_idx:\n",
    "            records.append({\"sample_id\": sample_ids[idx], \"fold\": fold_num, \"split\": \"val\"})\n",
    "\n",
    "    fold_splits_df = pd.DataFrame(records)\n",
    "    fold_splits_df.to_csv(output_path, index=False)\n",
    "    print(f\"  Saved -> {output_path}\")\n",
    "    return fold_splits_df\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING fold_splits.csv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if FOLD_SPLITS_PATH.exists() and not FORCE_REGENERATE_FOLDS:\n",
    "    print(f\"  [SKIP] Already exists - loading from disk.\")\n",
    "    print(f\"         Set FORCE_REGENERATE_FOLDS = True to rebuild.\")\n",
    "    fold_splits = pd.read_csv(FOLD_SPLITS_PATH)\n",
    "else:\n",
    "    fold_splits = generate_fold_splits(\n",
    "        master_index, FOLD_SPLITS_PATH, n_folds=N_FOLDS, seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "print(f\"\\n  Fold summary (sample level):\")\n",
    "for fold_num in range(1, N_FOLDS + 1):\n",
    "    fold_data   = fold_splits[fold_splits[\"fold\"] == fold_num]\n",
    "    train_ids   = set(fold_data[fold_data[\"split\"] == \"train\"][\"sample_id\"])\n",
    "    val_ids     = set(fold_data[fold_data[\"split\"] == \"val\"][\"sample_id\"])\n",
    "    train_patch = len(master_index[master_index[\"sample_id\"].isin(train_ids)])\n",
    "    val_patch   = len(master_index[master_index[\"sample_id\"].isin(val_ids)])\n",
    "    print(f\"  Fold {fold_num}: {len(train_ids):3d} train samples ({train_patch:,} patches) | \"\n",
    "          f\"{len(val_ids):3d} val samples ({val_patch:,} patches)\")\n",
    "\n",
    "print(\"\\n  Preview:\")\n",
    "display(fold_splits.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b11113",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 - Data Verification\n",
    "Runs all checks from the old `verify_data.py`. Stops early if anything is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "_checks_passed = True\n",
    "\n",
    "# ---- 1. Required columns -----------------------------------------------------\n",
    "print(\"\\n[1/4] Required Columns\")\n",
    "REQUIRED_COLS = [\"sample_id\", \"class\", \"path\", \"label_stage1\"]\n",
    "missing_cols  = [c for c in REQUIRED_COLS if c not in master_index.columns]\n",
    "if missing_cols:\n",
    "    print(f\"  [X] Missing columns: {missing_cols}\")\n",
    "    _checks_passed = False\n",
    "else:\n",
    "    print(\"  [OK] All required columns present\")\n",
    "\n",
    "# ---- 2. File existence -------------------------------------------------------\n",
    "print(\"\\n[2/4] .npy File Existence\")\n",
    "all_paths  = master_index[\"path\"].tolist()\n",
    "n_existing = sum(1 for p in all_paths if os.path.exists(p))\n",
    "n_missing  = len(all_paths) - n_existing\n",
    "\n",
    "print(f\"  Total   : {len(all_paths):,}\")\n",
    "print(f\"  [OK] Found  : {n_existing:,}\")\n",
    "\n",
    "if n_missing > 0:\n",
    "    print(f\"  [!] Missing : {n_missing:,}\")\n",
    "    for p in [p for p in all_paths if not os.path.exists(p)][:3]:\n",
    "        print(f\"       {p}\")\n",
    "    if n_existing == 0:\n",
    "        print(\"  [X] Zero files accessible - check your dataset folder.\")\n",
    "        _checks_passed = False\n",
    "    else:\n",
    "        print(f\"  [!] Trimming master_index to {n_existing:,} accessible rows.\")\n",
    "        master_index = master_index[master_index[\"path\"].apply(os.path.exists)].copy()\n",
    "else:\n",
    "    print(f\"  [OK] All files found\")\n",
    "\n",
    "# ---- 3. .npy format spot-check -----------------------------------------------\n",
    "print(\"\\n[3/4] .npy Format Spot-Check\")\n",
    "existing_paths = [p for p in master_index[\"path\"].tolist() if os.path.exists(p)]\n",
    "if existing_paths:\n",
    "    _s = np.load(existing_paths[0])\n",
    "    print(f\"  Shape      : {_s.shape}\")\n",
    "    print(f\"  dtype      : {_s.dtype}\")\n",
    "    print(f\"  Value range: [{_s.min():.3f}, {_s.max():.3f}]\")\n",
    "    print(f\"  {'[OK] Shape compatible' if _s.shape in [(64,64,3),(64,64)] else '[!] Unexpected shape - preprocess_npy will attempt to handle it'}\")\n",
    "    print(f\"  {'[OK] Normalised to [0,1]' if _s.max() <= 1.0 else '[!] Values in [0,255] - will normalise during loading'}\")\n",
    "else:\n",
    "    print(\"  [X] No files to spot-check\")\n",
    "    _checks_passed = False\n",
    "\n",
    "# ---- 4. Fold leakage check ---------------------------------------------------\n",
    "print(\"\\n[4/4] Fold Splits & Leakage Check\")\n",
    "_leakage_found = False\n",
    "for fold_num in range(1, N_FOLDS + 1):\n",
    "    fold_data = fold_splits[fold_splits[\"fold\"] == fold_num]\n",
    "    train_ids = set(fold_data[fold_data[\"split\"] == \"train\"][\"sample_id\"])\n",
    "    val_ids   = set(fold_data[fold_data[\"split\"] == \"val\"][\"sample_id\"])\n",
    "    overlap   = train_ids & val_ids\n",
    "    train_pat = len(master_index[master_index[\"sample_id\"].isin(train_ids)])\n",
    "    val_pat   = len(master_index[master_index[\"sample_id\"].isin(val_ids)])\n",
    "    status    = \"[OK]\" if not overlap else \"[X] LEAKAGE DETECTED\"\n",
    "    print(f\"  Fold {fold_num}: {len(train_ids):3d} train samples ({train_pat:,} patches) | \"\n",
    "          f\"{len(val_ids):3d} val samples ({val_pat:,} patches)  {status}\")\n",
    "    if overlap:\n",
    "        print(f\"    Overlapping IDs: {list(overlap)[:5]}\")\n",
    "        _leakage_found = True\n",
    "\n",
    "if _leakage_found:\n",
    "    print(\"  [X] Leakage detected - set FORCE_REGENERATE_FOLDS=True in Cell 5 and re-run.\")\n",
    "    _checks_passed = False\n",
    "\n",
    "# ---- Sample visualisation ----------------------------------------------------\n",
    "print(\"\\n[Visual] Sample Patches per Class\")\n",
    "try:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    for ax, cls in zip(axes, [\"PF\", \"CF\", \"NF\"]):\n",
    "        cls_paths = master_index[master_index[\"class\"] == cls][\"path\"].tolist()\n",
    "        hit = next((p for p in cls_paths if os.path.exists(p)), None)\n",
    "        if hit:\n",
    "            img = np.load(hit)\n",
    "            if img.ndim == 2:\n",
    "                img = np.stack([img] * 3, axis=-1)\n",
    "            if img.max() > 1.0:\n",
    "                img = img / 255.0\n",
    "            ax.imshow(np.clip(img, 0, 1))\n",
    "            ax.set_title(cls, fontsize=13, fontweight='bold')\n",
    "        else:\n",
    "            ax.set_title(f\"{cls} (no file)\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(\"One sample patch per class\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    vis_path = OUTPUT_DIR / \"verification_samples.png\"\n",
    "    plt.savefig(vis_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"  Saved to {vis_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"  [!] Could not render visualisation: {e}\")\n",
    "\n",
    "# ---- Summary -----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if _checks_passed:\n",
    "    print(\"[OK] ALL CHECKS PASSED - ready to train!\")\n",
    "else:\n",
    "    print(\"[X]  SOME CHECKS FAILED - fix the issues above before continuing.\")\n",
    "    raise RuntimeError(\"Data verification failed. See output above.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef6512",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 - Data Loading & Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold_data(fold_num: int):\n",
    "    \"\"\"\n",
    "    Return (train_df, val_df) for the given fold.\n",
    "    Splits at the sample level so all patches from one sample\n",
    "    always stay in the same split - no leakage possible.\n",
    "    \"\"\"\n",
    "    fold_data = fold_splits[fold_splits[\"fold\"] == fold_num]\n",
    "    train_ids = fold_data[fold_data[\"split\"] == \"train\"][\"sample_id\"].tolist()\n",
    "    val_ids   = fold_data[fold_data[\"split\"] == \"val\"][\"sample_id\"].tolist()\n",
    "\n",
    "    train_df  = master_index[master_index[\"sample_id\"].isin(train_ids)].copy()\n",
    "    val_df    = master_index[master_index[\"sample_id\"].isin(val_ids)].copy()\n",
    "\n",
    "    print(f\"  Fold {fold_num}: {len(train_ids)} train samples ({len(train_df):,} patches) | \"\n",
    "          f\"{len(val_ids)} val samples ({len(val_df):,} patches)\")\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def preprocess_npy(npy_path) -> np.ndarray:\n",
    "    \"\"\"Load a .npy patch and return a normalised (64, 64, 3) float32 array.\"\"\"\n",
    "    img = np.load(npy_path)\n",
    "\n",
    "    # Already ImageNet-normalised (has negative values)\n",
    "    if img.min() < 0:\n",
    "        img = img.astype(np.float32)\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img] * 3, axis=-1)\n",
    "        elif img.shape[-1] == 1:\n",
    "            img = np.repeat(img, 3, axis=-1)\n",
    "        return img\n",
    "\n",
    "    # Ensure 3-channel\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "    elif img.shape[-1] == 1:\n",
    "        img = np.repeat(img, 3, axis=-1)\n",
    "\n",
    "    # Resize if needed\n",
    "    if img.shape[:2] != (64, 64):\n",
    "        img = cv2.resize(img.astype(np.float32), (64, 64))\n",
    "\n",
    "    # Normalise to [0, 1]\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    # ImageNet mean/std normalisation\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    return (img - mean) / std\n",
    "\n",
    "\n",
    "print(\"[OK] Data utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81547b",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 - Generator & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81703db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"On-the-fly .npy data generator.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame, batch_size: int = 32,\n",
    "                 shuffle: bool = True):\n",
    "        self.df         = dataframe.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.n          = len(self.df)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(self.n / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        start = index * self.batch_size\n",
    "        end   = min(start + self.batch_size, self.n)\n",
    "        rows  = self.df.iloc[self.indices[start:end]]\n",
    "        X     = np.array([preprocess_npy(p) for p in rows[\"path\"]])\n",
    "        y     = keras.utils.to_categorical(rows[\"label_stage1\"].values, num_classes=NUM_CLASSES)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def reset(self):\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "def build_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES) -> keras.Model:\n",
    "    \"\"\"EfficientNetB3 backbone with a classification head.\"\"\"\n",
    "    try:\n",
    "        base = EfficientNetB3(include_top=False, weights=\"imagenet\",\n",
    "                              input_shape=input_shape, pooling=\"avg\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not load ImageNet weights, using random init: {e}\")\n",
    "        base = EfficientNetB3(include_top=False, weights=None,\n",
    "                              input_shape=input_shape, pooling=\"avg\")\n",
    "\n",
    "    base.trainable = False  # frozen backbone - transfer learning\n",
    "\n",
    "    inputs  = layers.Input(shape=input_shape)\n",
    "    x       = base(inputs, training=False)\n",
    "    x       = layers.Dropout(0.3)(x)\n",
    "    x       = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x       = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"EfficientNetB3_stage1\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "_test = build_model()\n",
    "_test.summary()\n",
    "del _test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354f4ce",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 - Evaluation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred_proba, threshold=THRESHOLD, class_names=CLASS_NAMES):\n",
    "    y_true       = np.asarray(y_true)\n",
    "    y_pred_proba = np.asarray(y_pred_proba)\n",
    "\n",
    "    if y_pred_proba.ndim > 1:\n",
    "        y_pred_pos = y_pred_proba[:, POSITIVE_CLASS_INDEX]\n",
    "        y_pred_cls = np.argmax(y_pred_proba, axis=1)\n",
    "    else:\n",
    "        y_pred_pos = y_pred_proba\n",
    "        y_pred_cls = (y_pred_pos >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_cls)\n",
    "    if cm.shape != (2, 2):\n",
    "        raise ValueError(f\"Expected (2,2) confusion matrix, got {cm.shape}\")\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity  = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    precision    = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    accuracy     = (tp + tn) / (tp + tn + fp + fn)\n",
    "    balanced_acc = 0.5 * (sensitivity + specificity)\n",
    "    f1           = (2 * precision * sensitivity / (precision + sensitivity)\n",
    "                    if (precision + sensitivity) > 0 else 0.0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_pos)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Classes           : {class_names}\")\n",
    "    print(f\"\\nConfusion matrix:\\n{cm}\")\n",
    "    print(f\"\\nSensitivity (TPR) : {sensitivity:.4f}\")\n",
    "    print(f\"Specificity (TNR) : {specificity:.4f}\")\n",
    "    print(f\"Balanced accuracy : {balanced_acc:.4f}\")\n",
    "    print(f\"Accuracy          : {accuracy:.4f}\")\n",
    "    print(f\"Precision (PPV)   : {precision:.4f}\")\n",
    "    print(f\"F1-score          : {f1:.4f}\")\n",
    "    print(f\"AUC-ROC           : {auc:.4f}\" if not np.isnan(auc) else \"AUC-ROC           : N/A\")\n",
    "\n",
    "    return dict(sensitivity=sensitivity, specificity=specificity,\n",
    "                balanced_accuracy=balanced_acc, accuracy=accuracy,\n",
    "                precision=precision, f1_score=f1, auc=auc,\n",
    "                tp=int(tp), tn=int(tn), fp=int(fp), fn=int(fn))\n",
    "\n",
    "\n",
    "print(\"[OK] Evaluation helper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6d463",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 - Training Loop (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for fold_num in range(1, N_FOLDS + 1):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"FOLD {fold_num} / {N_FOLDS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fold_output_dir = OUTPUT_DIR / f\"fold{fold_num}\"\n",
    "    fold_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_df, val_df = load_fold_data(fold_num)\n",
    "\n",
    "    # Class weights to handle imbalance\n",
    "    y_train       = train_df[\"label_stage1\"].values\n",
    "    classes       = np.unique(y_train)\n",
    "    cw_arr        = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "    class_weights = {int(c): float(w) for c, w in zip(classes, cw_arr)}\n",
    "    print(f\"  Class weights: {class_weights}\")\n",
    "\n",
    "    train_gen = NumpyDataGenerator(train_df, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_gen   = NumpyDataGenerator(val_df,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_model()\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(fold_output_dir / \"best_model.h5\"),\n",
    "            monitor=\"val_accuracy\", mode=\"max\",\n",
    "            save_best_only=True, verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\", patience=5,\n",
    "            restore_best_weights=True, verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.CSVLogger(str(fold_output_dir / \"history.csv\")),\n",
    "    ]\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"  Training time: {elapsed:.0f}s\")\n",
    "\n",
    "    val_gen.reset()\n",
    "    y_pred_proba = model.predict(val_gen, verbose=0)\n",
    "    y_true       = val_df[\"label_stage1\"].values\n",
    "\n",
    "    metrics = evaluate_predictions(y_true, y_pred_proba)\n",
    "    metrics[\"fold\"]          = fold_num\n",
    "    metrics[\"train_patches\"] = int(len(train_df))\n",
    "    metrics[\"val_patches\"]   = int(len(val_df))\n",
    "    metrics[\"train_time_s\"]  = int(elapsed)\n",
    "    all_results.append(metrics)\n",
    "\n",
    "    # Save incrementally so a crash doesn't lose earlier folds\n",
    "    pd.DataFrame(all_results).to_csv(OUTPUT_DIR / \"all_results.csv\", index=False)\n",
    "    print(f\"  Results saved -> {OUTPUT_DIR / 'all_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3eff44",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 - Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e79680",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df  = pd.DataFrame(all_results)\n",
    "metric_cols = [\"sensitivity\", \"specificity\", \"balanced_accuracy\", \"accuracy\", \"f1_score\", \"auc\"]\n",
    "summary     = results_df[metric_cols].agg([\"mean\", \"std\"]).T\n",
    "summary.columns = [\"Mean\", \"Std\"]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(summary.to_string(float_format=\"{:.4f}\".format))\n",
    "print(\"\\nPer-fold results:\")\n",
    "display(results_df[[\"fold\"] + metric_cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
