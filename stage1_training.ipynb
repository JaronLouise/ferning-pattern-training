{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 1 Ferning Classification – Simple Notebook Training\n",
        "\n",
        "This notebook is a simplified, self-contained version of the Stage 1 training pipeline.\n",
        "\n",
        "It:\n",
        "- Loads the existing master index and fold splits CSVs\n",
        "- Builds a small EfficientNetB3-based model\n",
        "- Trains on one cross-validation fold using a Numpy data generator\n",
        "- Computes basic medical metrics (sensitivity, specificity, balanced accuracy, AUC)\n",
        "\n",
        "Run the cells from top to bottom to train and evaluate the model on your local data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "\n",
        "# Basic TF / GPU setup\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if physical_gpus:\n",
        "    for gpu in physical_gpus:\n",
        "        try:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not set memory growth on GPU: {e}\")\n",
        "    print(f\"Using GPU(s): {[g.name for g in physical_gpus]}\")\n",
        "else:\n",
        "    print(\"Using CPU (training will be slower)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration – adjust these as needed\n",
        "\n",
        "# Root of this simple repo (this notebook assumes it lives in simple_notebook_repo/)\n",
        "REPO_ROOT = Path.cwd()\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "\n",
        "# Paths to your existing data (reusing the same CSVs used by the original project)\n",
        "MASTER_INDEX_PATH = REPO_ROOT.parent / \"local\" / \"data\" / \"master_patch_index.csv\"\n",
        "FOLD_SPLITS_PATH = REPO_ROOT.parent / \"local\" / \"data\" / \"fold_splits.csv\"\n",
        "\n",
        "# Where to save notebook outputs\n",
        "OUTPUT_DIR = REPO_ROOT / \"outputs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "INPUT_SHAPE = (64, 64, 3)\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Cross-validation settings (you can lower this to 1 while experimenting)\n",
        "N_FOLDS = 5\n",
        "\n",
        "# Class / evaluation config\n",
        "CLASS_NAMES = [\"No Ferning\", \"Ferning\"]\n",
        "POSITIVE_CLASS_INDEX = 1\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "print(\"Master index:\", MASTER_INDEX_PATH)\n",
        "print(\"Fold splits :\", FOLD_SPLITS_PATH)\n",
        "print(\"Output dir  :\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading utilities\n",
        "\n",
        "assert MASTER_INDEX_PATH.exists(), f\"Master index not found: {MASTER_INDEX_PATH}\"\n",
        "assert FOLD_SPLITS_PATH.exists(), f\"Fold splits not found: {FOLD_SPLITS_PATH}\"\n",
        "\n",
        "master_index = pd.read_csv(MASTER_INDEX_PATH)\n",
        "fold_splits = pd.read_csv(FOLD_SPLITS_PATH)\n",
        "\n",
        "print(f\"Loaded master index with {len(master_index)} patches from {master_index['sample_id'].nunique()} samples\")\n",
        "print(f\"Loaded fold splits with {len(fold_splits)} rows\")\n",
        "\n",
        "\n",
        "def remap_paths(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Remap old absolute paths in the CSV to the local repo dataset folder.\"\"\"\n",
        "    df = df.copy()\n",
        "    if \"path\" in df.columns:\n",
        "        old_prefix = \"C:/Users/Jaron/Downloads/dataset/dataset\"\n",
        "        new_prefix = str((REPO_ROOT.parent / \"local\" / \"data\" / \"dataset\" / \"dataset\").resolve())\n",
        "        df[\"path\"] = (\n",
        "            df[\"path\"]\n",
        "            .astype(str)\n",
        "            .str.replace(old_prefix, new_prefix, regex=False)\n",
        "            .str.replace(\"\\\\\", \"/\", regex=False)\n",
        "        )\n",
        "    return df\n",
        "\n",
        "\n",
        "master_index = remap_paths(master_index)\n",
        "\n",
        "# Ensure label_stage1 exists\n",
        "if \"label_stage1\" not in master_index.columns:\n",
        "    master_index[\"label_stage1\"] = master_index[\"class\"].apply(\n",
        "        lambda x: 1 if x in [\"PF\", \"CF\"] else 0\n",
        "    )\n",
        "\n",
        "\n",
        "def load_fold_data_simple(fold_num: int):\n",
        "    \"\"\"Return train/val DataFrames for the given fold number.\"\"\"\n",
        "    fold_data = fold_splits[fold_splits[\"fold\"] == fold_num]\n",
        "    train_samples = fold_data[fold_data[\"split\"] == \"train\"][\"sample_id\"].tolist()\n",
        "    val_samples = fold_data[fold_data[\"split\"] == \"val\"][\"sample_id\"].tolist()\n",
        "\n",
        "    train_df = master_index[master_index[\"sample_id\"].isin(train_samples)].copy()\n",
        "    val_df = master_index[master_index[\"sample_id\"].isin(val_samples)].copy()\n",
        "\n",
        "    print(f\"Fold {fold_num}: train={len(train_df)} patches, val={len(val_df)} patches\")\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def preprocess_npy(npy_path: str | Path) -> np.ndarray:\n",
        "    \"\"\"Load and preprocess a .npy image file into (64, 64, 3) float32.\"\"\"\n",
        "    img_array = np.load(npy_path)\n",
        "\n",
        "    # Detect if already preprocessed\n",
        "    is_preprocessed = (img_array.min() < 0) or (img_array.max() > 255)\n",
        "\n",
        "    if is_preprocessed:\n",
        "        img_array = img_array.astype(np.float32)\n",
        "        if img_array.ndim == 2:\n",
        "            img_array = np.stack([img_array] * 3, axis=-1)\n",
        "        elif img_array.shape[-1] == 1:\n",
        "            img_array = np.repeat(img_array, 3, axis=-1)\n",
        "        return img_array\n",
        "\n",
        "    # Ensure 3D (H, W, C)\n",
        "    if img_array.ndim == 2:\n",
        "        img_array = np.stack([img_array] * 3, axis=-1)\n",
        "\n",
        "    # Resize to (64, 64)\n",
        "    if img_array.shape[:2] != (64, 64):\n",
        "        img_array = cv2.resize(img_array, (64, 64))\n",
        "\n",
        "    # Ensure RGB\n",
        "    if img_array.ndim == 2:\n",
        "        img_array = np.stack([img_array] * 3, axis=-1)\n",
        "    elif img_array.shape[-1] == 1:\n",
        "        img_array = np.repeat(img_array, 3, axis=-1)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    if img_array.max() > 1.0:\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "    img_array = img_array.astype(np.float32)\n",
        "\n",
        "    # ImageNet normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "    img_array = (img_array - mean) / std\n",
        "\n",
        "    return img_array\n",
        "\n",
        "\n",
        "class NumpyDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Minimal .npy-based generator with on-the-fly loading.\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe: pd.DataFrame, batch_size: int = 32,\n",
        "                 shuffle: bool = True, augment: bool = False):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment  # kept for API compatibility, not used here\n",
        "        self.n = len(self.df)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(np.ceil(self.n / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = min((index + 1) * self.batch_size, self.n)\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "        batch_df = self.df.iloc[batch_indices]\n",
        "\n",
        "        X = np.array([preprocess_npy(path) for path in batch_df[\"path\"]])\n",
        "\n",
        "        y = keras.utils.to_categorical(\n",
        "            batch_df[\"label_stage1\"].values,\n",
        "            num_classes=NUM_CLASSES,\n",
        "        )\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(self.n)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def reset(self):\n",
        "        self.on_epoch_end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition – EfficientNetB3 backbone with simple head\n",
        "\n",
        "\n",
        "def build_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
        "    try:\n",
        "        base = EfficientNetB3(\n",
        "            include_top=False,\n",
        "            weights=\"imagenet\",\n",
        "            input_shape=input_shape,\n",
        "            pooling=\"avg\",\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load ImageNet weights, using random init: {e}\")\n",
        "        base = EfficientNetB3(\n",
        "            include_top=False,\n",
        "            weights=None,\n",
        "            input_shape=input_shape,\n",
        "            pooling=\"avg\",\n",
        "        )\n",
        "\n",
        "    # Freeze backbone for simple transfer learning\n",
        "    base.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = base(inputs, training=False)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs, name=\"EfficientNetB3_simple\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.summary()}},{"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation helpers – confusion matrix + medical-style metrics\n",
        "\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred_proba, threshold=THRESHOLD, class_names=CLASS_NAMES):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred_proba = np.asarray(y_pred_proba)\n",
        "\n",
        "    if y_pred_proba.ndim > 1:\n",
        "        y_pred_pos = y_pred_proba[:, POSITIVE_CLASS_INDEX]\n",
        "        y_pred_cls = np.argmax(y_pred_proba, axis=1)\n",
        "    else:\n",
        "        y_pred_pos = y_pred_proba\n",
        "        y_pred_cls = (y_pred_pos >= threshold).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_cls)\n",
        "    if cm.shape != (2, 2):\n",
        "        raise ValueError(f\"Expected binary confusion matrix, got shape {cm.shape}\")\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    balanced_acc = 0.5 * (sensitivity + specificity)\n",
        "    f1 = (\n",
        "        2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "        if (precision + sensitivity) > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_pred_pos)\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EVALUATION REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Classes: {class_names}\")\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\nPrimary metrics (medical):\")\n",
        "    print(f\"Sensitivity (recall / TPR): {sensitivity:.4f}\")\n",
        "    print(f\"Specificity (TNR)        : {specificity:.4f}\")\n",
        "    print(f\"Balanced accuracy        : {balanced_acc:.4f}\")\n",
        "    print(\"\\nAdditional metrics:\")\n",
        "    print(f\"Accuracy                 : {accuracy:.4f}\")\n",
        "    print(f\"Precision (PPV)          : {precision:.4f}\")\n",
        "    print(f\"F1-score                 : {f1:.4f}\")\n",
        "    print(f\"AUC-ROC                  : {auc:.4f}\" if not np.isnan(auc) else \"AUC-ROC: N/A\")\n",
        "\n",
        "    return {\n",
        "        \"sensitivity\": sensitivity,\n",
        "        \"specificity\": specificity,\n",
        "        \"balanced_accuracy\": balanced_acc,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"f1_score\": f1,\n",
        "        \"auc\": auc,\n",
        "        \"tp\": int(tp),\n",
        "        \"tn\": int(tn),\n",
        "        \"fp\": int(fp),\n",
        "        \"fn\": int(fn),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Main training loop over folds\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for fold_num in range(1, N_FOLDS + 1):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"FOLD {fold_num}/{N_FOLDS}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    fold_output_dir = OUTPUT_DIR / f\"fold{fold_num}\"\n",
        "    fold_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Prepare data\n",
        "    train_df, val_df = load_fold_data_simple(fold_num)\n",
        "\n",
        "    # Class weights for imbalance\n",
        "    y_train = train_df[\"label_stage1\"].values\n",
        "    classes = np.unique(y_train)\n",
        "    class_weights_arr = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "    class_weights = {int(c): float(w) for c, w in zip(classes, class_weights_arr)}\n",
        "    print(\"Class weights:\", class_weights)\n",
        "\n",
        "    train_gen = NumpyDataGenerator(train_df, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_gen = NumpyDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Build a fresh model for this fold\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_model()\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            filepath=str(fold_output_dir / \"best_model.h5\"),\n",
        "            monitor=\"val_accuracy\",\n",
        "            mode=\"max\",\n",
        "            save_best_only=True,\n",
        "            verbose=1,\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\",\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "        ),\n",
        "        keras.callbacks.CSVLogger(str(fold_output_dir / \"history.csv\")),\n",
        "    ]\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=EPOCHS,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "    elapsed = (datetime.now() - start_time).total_seconds()\n",
        "    print(f\"Training time: {elapsed:.0f}s\")\n",
        "\n",
        "    # Evaluation\n",
        "    val_gen.reset()\n",
        "    y_pred_proba = model.predict(val_gen, verbose=0)\n",
        "    y_true = val_df[\"label_stage1\"].values\n",
        "\n",
        "    metrics_dict = evaluate_predictions(\n",
        "        y_true=y_true,\n",
        "        y_pred_proba=y_pred_proba,\n",
        "        threshold=THRESHOLD,\n",
        "        class_names=CLASS_NAMES,\n",
        "    )\n",
        "\n",
        "    metrics_dict[\"fold\"] = fold_num\n",
        "    metrics_dict[\"train_samples\"] = int(len(train_df))\n",
        "    metrics_dict[\"val_samples\"] = int(len(val_df))\n",
        "\n",
        "    all_results.append(metrics_dict)\n",
        "\n",
        "    # Save incremental results\n",
        "    pd.DataFrame(all_results).to_csv(OUTPUT_DIR / \"all_results.csv\", index=False)\n",
        "\n",
        "# Summary across folds\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
